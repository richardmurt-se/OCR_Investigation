{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nemotron-Parse Invoice Processing\n",
        "\n",
        "> **WARNING: This notebook requires 24+ GB VRAM and will NOT run on Kaggle's free GPU tier (P100 16GB).**\n",
        ">\n",
        "> The model's ViT-H vision encoder requires ~10.5 GB for attention computation at the fixed 2048x1664 input resolution. This cannot be reduced without retraining the model. See `docs/nemotron_parse.md` for full details on workarounds attempted.\n",
        ">\n",
        "> **Recommended GPUs:** NVIDIA A100 (40GB+), H100 (80GB), A10 (24GB), or RTX 4090 (24GB)\n",
        "\n",
        "This notebook processes invoice images using NVIDIA Nemotron-Parse v1.1 (885M) with GPU acceleration.\n",
        "Uses the HuggingFace Transformers API with the `nvidia/NVIDIA-Nemotron-Parse-v1.1` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Install dependencies\n",
        "# Nemotron-Parse requires pinned transformers==4.51.3 (issues with 4.53+)\n",
        "# open_clip_torch is required by the C-RADIO vision encoder\n",
        "# pypdfium2 is needed for PDF-to-image conversion (model works on images only)\n",
        "!pip install transformers==4.51.3 --quiet\n",
        "!pip install accelerate==1.12.0 albumentations==2.0.8 timm==1.0.22 --quiet\n",
        "!pip install open_clip_torch --quiet\n",
        "!pip install pypdfium2 Pillow --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Verify GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Setup and initialize Nemotron-Parse\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from transformers import AutoModel, AutoProcessor, AutoTokenizer, GenerationConfig\n",
        "\n",
        "# Reduce VRAM fragmentation on T4 GPU\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Configuration\n",
        "INPUT_DIR = Path(\"/kaggle/input/synthetic-invoices\")\n",
        "OUTPUT_DIR = Path(\"/kaggle/working/nemotron-parse\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"raw\").mkdir(exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"nvidia/NVIDIA-Nemotron-Parse-v1.1\"\n",
        "DEVICE = \"cuda:0\"\n",
        "\n",
        "# Initialize Nemotron-Parse\n",
        "# - trust_remote_code=True: required for custom model architecture\n",
        "# - BF16 precision: optimal for this model\n",
        "print(\"Loading Nemotron-Parse model...\")\n",
        "model = AutoModel.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16\n",
        ").to(DEVICE).eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "print(f\"Nemotron-Parse loaded on {DEVICE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Define processing function\n",
        "import pypdfium2 as pdfium\n",
        "import tempfile\n",
        "from PIL import Image\n",
        "\n",
        "def pdf_to_image_path(pdf_path):\n",
        "    \"\"\"Convert first page of a PDF to a temporary PNG image. Returns the temp file path.\"\"\"\n",
        "    pdf = pdfium.PdfDocument(str(pdf_path))\n",
        "    page = pdf[0]\n",
        "    bitmap = page.render(scale=2)  # Higher scale for better OCR quality\n",
        "    pil_image = bitmap.to_pil()\n",
        "    tmp = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
        "    tmp_path = tmp.name\n",
        "    tmp.close()\n",
        "    pil_image.save(tmp_path)\n",
        "    return tmp_path\n",
        "\n",
        "def process_image(image_path, model, processor, generation_config):\n",
        "    \"\"\"Process a single image or PDF with Nemotron-Parse.\"\"\"\n",
        "    start_time = time.time()\n",
        "    temp_file = None\n",
        "\n",
        "    try:\n",
        "        # Convert PDF to image if needed\n",
        "        if str(image_path).lower().endswith(\".pdf\"):\n",
        "            temp_file = pdf_to_image_path(image_path)\n",
        "            image = Image.open(temp_file)\n",
        "        else:\n",
        "            image = Image.open(str(image_path))\n",
        "\n",
        "        # Task prompt for markdown output with bboxes and classes\n",
        "        task_prompt = \"</s><s><predict_bbox><predict_classes><output_markdown>\"\n",
        "\n",
        "        # Process image\n",
        "        inputs = processor(\n",
        "            images=[image],\n",
        "            text=task_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=False\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Generate text\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"raw_text\": generated_text,\n",
        "            \"raw_data\": [{\"text\": generated_text}] if generated_text else [],\n",
        "            \"processing_time_seconds\": round(processing_time, 3)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": f\"{str(e)}\\n{traceback.format_exc()}\",\n",
        "            \"raw_text\": \"\",\n",
        "            \"raw_data\": [],\n",
        "            \"processing_time_seconds\": time.time() - start_time\n",
        "        }\n",
        "    finally:\n",
        "        # Free GPU memory between images to prevent OOM\n",
        "        torch.cuda.empty_cache()\n",
        "        # Clean up temp PDF-converted image\n",
        "        if temp_file and os.path.exists(temp_file):\n",
        "            os.unlink(temp_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Test on a single image before batch processing\n",
        "image_files = sorted(\n",
        "    list(INPUT_DIR.glob(\"*.png\")) +\n",
        "    list(INPUT_DIR.glob(\"*.jpeg\")) +\n",
        "    list(INPUT_DIR.glob(\"*.pdf\"))\n",
        ")\n",
        "print(f\"Found {len(image_files)} files total\")\n",
        "\n",
        "if image_files:\n",
        "    test_img = image_files[0]\n",
        "    print(f\"\\nTesting on: {test_img.name}\")\n",
        "\n",
        "    test_result = process_image(test_img, model, processor, generation_config)\n",
        "    print(f\"Success: {test_result['success']}\")\n",
        "    print(f\"Processing time: {test_result['processing_time_seconds']:.2f}s\")\n",
        "\n",
        "    if test_result[\"raw_text\"]:\n",
        "        preview = test_result[\"raw_text\"][:500]\n",
        "        print(f\"Text preview ({len(test_result['raw_text'])} chars):\\n{preview}\")\n",
        "        print(\"\\nOCR pipeline is working correctly!\")\n",
        "    else:\n",
        "        print(\"\\nWARNING: No text extracted!\")\n",
        "        if test_result.get(\"error\"):\n",
        "            print(f\"Error: {test_result['error']}\")\n",
        "else:\n",
        "    print(\"ERROR: No images found in input directory!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: Process all images\n",
        "print(f\"Processing {len(image_files)} files...\")\n",
        "\n",
        "empty_count = 0\n",
        "results = []\n",
        "for i, image_path in enumerate(image_files):\n",
        "    print(f\"Processing {i+1}/{len(image_files)}: {image_path.name}\", end=\"\")\n",
        "\n",
        "    result = process_image(image_path, model, processor, generation_config)\n",
        "\n",
        "    if not result.get(\"raw_text\"):\n",
        "        empty_count += 1\n",
        "        print(\" - WARNING: no text extracted!\", end=\"\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Create output structure\n",
        "    output = {\n",
        "        \"filename\": image_path.name,\n",
        "        \"model_name\": \"nemotron-parse\",\n",
        "        \"raw_text\": result.get(\"raw_text\", \"\"),\n",
        "        \"processing_time_seconds\": result.get(\"processing_time_seconds\", 0),\n",
        "        \"file_size_bytes\": image_path.stat().st_size,\n",
        "        \"success\": result.get(\"success\", False)\n",
        "    }\n",
        "\n",
        "    if not result[\"success\"]:\n",
        "        output[\"error\"] = result.get(\"error\", \"Unknown error\")\n",
        "\n",
        "    results.append(output)\n",
        "\n",
        "    # Save individual result\n",
        "    result_file = OUTPUT_DIR / f\"{image_path.stem}_{image_path.suffix.lstrip('.')}.json\"\n",
        "    with open(result_file, \"w\") as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "\n",
        "    # Save raw response\n",
        "    raw_file = OUTPUT_DIR / \"raw\" / f\"{image_path.stem}_{image_path.suffix.lstrip('.')}_raw.json\"\n",
        "    with open(raw_file, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"content\": result.get(\"raw_text\", \"\"),\n",
        "            \"detections\": result.get(\"raw_data\", [])\n",
        "        }, f, indent=2)\n",
        "\n",
        "print(f\"\\nCompleted! Processed {len(results)} files\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 7: Save combined results and summary\n",
        "all_results_file = OUTPUT_DIR / \"all_results.json\"\n",
        "with open(all_results_file, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Print summary\n",
        "successful = sum(1 for r in results if r.get(\"success\", False))\n",
        "with_text = sum(1 for r in results if r.get(\"raw_text\", \"\"))\n",
        "total_time = sum(r.get(\"processing_time_seconds\", 0) for r in results)\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Total files: {len(results)}\")\n",
        "print(f\"  Successful: {successful}\")\n",
        "print(f\"  With text extracted: {with_text}\")\n",
        "print(f\"  Empty results: {empty_count}\")\n",
        "print(f\"  Failed: {len(results) - successful}\")\n",
        "print(f\"  Total processing time: {total_time:.1f}s\")\n",
        "print(f\"  Average time per file: {total_time/len(results):.2f}s\")\n",
        "print(f\"\\nResults saved to: {OUTPUT_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 8: Create downloadable ZIP\n",
        "import shutil\n",
        "\n",
        "shutil.make_archive(\n",
        "    \"/kaggle/working/nemotron_parse\",\n",
        "    'zip',\n",
        "    OUTPUT_DIR\n",
        ")\n",
        "print(\"ZIP created: /kaggle/working/nemotron_parse.zip\")\n",
        "print(\"Download from the Output tab after notebook completes.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}