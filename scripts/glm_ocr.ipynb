{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GLM-OCR Invoice Processing\n",
        "\n",
        "This notebook processes invoice images using GLM-OCR (0.9B) with GPU acceleration on Kaggle.\n",
        "Uses the HuggingFace Transformers API with the `zai-org/GLM-OCR` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies\n",
        "# GLM-OCR requires transformers from GitHub (not PyPI release)\n",
        "# pypdfium2 is needed for PDF-to-image conversion (GLM-OCR works on images only)\n",
        "!pip install torch torchvision --quiet\n",
        "!pip install git+https://github.com/huggingface/transformers.git --quiet\n",
        "!pip install accelerate sentencepiece pypdfium2 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Verify GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Setup and initialize GLM-OCR\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "# Reduce VRAM fragmentation on T4 GPU\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Configuration\n",
        "INPUT_DIR = Path(\"/kaggle/input/synthetic-invoices\")\n",
        "OUTPUT_DIR = Path(\"/kaggle/working/glm-ocr\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"raw\").mkdir(exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"zai-org/GLM-OCR\"\n",
        "\n",
        "# Initialize GLM-OCR\n",
        "print(\"Loading GLM-OCR model...\")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(f\"GLM-OCR loaded on {model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Define processing function\n",
        "import pypdfium2 as pdfium\n",
        "import tempfile\n",
        "\n",
        "def pdf_to_image_path(pdf_path):\n",
        "    \"\"\"Convert first page of a PDF to a temporary PNG image. Returns the temp file path.\"\"\"\n",
        "    pdf = pdfium.PdfDocument(str(pdf_path))\n",
        "    page = pdf[0]\n",
        "    bitmap = page.render(scale=1)\n",
        "    pil_image = bitmap.to_pil()\n",
        "    tmp = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
        "    tmp_path = tmp.name\n",
        "    tmp.close()\n",
        "    pil_image.save(tmp_path)\n",
        "    return tmp_path\n",
        "\n",
        "def process_image(image_path, model, processor):\n",
        "    \"\"\"Process a single image or PDF with GLM-OCR.\"\"\"\n",
        "    start_time = time.time()\n",
        "    temp_file = None\n",
        "\n",
        "    try:\n",
        "        # Convert PDF to image if needed\n",
        "        if str(image_path).lower().endswith(\".pdf\"):\n",
        "            temp_file = pdf_to_image_path(image_path)\n",
        "            img_url = temp_file\n",
        "        else:\n",
        "            img_url = str(image_path)\n",
        "\n",
        "        # Build chat message with image and OCR prompt\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"url\": img_url},\n",
        "                    {\"type\": \"text\", \"text\": \"Text Recognition:\"}\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        inputs = processor.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_dict=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        inputs.pop(\"token_type_ids\", None)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(**inputs, max_new_tokens=8192)\n",
        "\n",
        "        output_text = processor.decode(\n",
        "            generated_ids[0][inputs[\"input_ids\"].shape[1]:],\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"raw_text\": output_text,\n",
        "            \"raw_data\": [{\"text\": output_text}] if output_text else [],\n",
        "            \"processing_time_seconds\": round(processing_time, 3)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": f\"{str(e)}\\n{traceback.format_exc()}\",\n",
        "            \"raw_text\": \"\",\n",
        "            \"raw_data\": [],\n",
        "            \"processing_time_seconds\": time.time() - start_time\n",
        "        }\n",
        "    finally:\n",
        "        # Free GPU memory between images to prevent OOM on PDFs\n",
        "        torch.cuda.empty_cache()\n",
        "        # Clean up temp PDF-converted image\n",
        "        if temp_file and os.path.exists(temp_file):\n",
        "            os.unlink(temp_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Test on a single image before batch processing\n",
        "image_files = sorted(\n",
        "    list(INPUT_DIR.glob(\"*.png\")) +\n",
        "    list(INPUT_DIR.glob(\"*.jpeg\")) +\n",
        "    list(INPUT_DIR.glob(\"*.pdf\"))\n",
        ")\n",
        "print(f\"Found {len(image_files)} files total\")\n",
        "\n",
        "if image_files:\n",
        "    test_img = image_files[0]\n",
        "    print(f\"\\nTesting on: {test_img.name}\")\n",
        "\n",
        "    test_result = process_image(test_img, model, processor)\n",
        "    print(f\"Success: {test_result['success']}\")\n",
        "    print(f\"Processing time: {test_result['processing_time_seconds']:.2f}s\")\n",
        "\n",
        "    if test_result[\"raw_text\"]:\n",
        "        preview = test_result[\"raw_text\"][:500]\n",
        "        print(f\"Text preview ({len(test_result['raw_text'])} chars):\\n{preview}\")\n",
        "        print(\"\\nOCR pipeline is working correctly!\")\n",
        "    else:\n",
        "        print(\"\\nWARNING: No text extracted!\")\n",
        "        if test_result.get(\"error\"):\n",
        "            print(f\"Error: {test_result['error']}\")\n",
        "else:\n",
        "    print(\"ERROR: No images found in input directory!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Process all images\n",
        "print(f\"Processing {len(image_files)} files...\")\n",
        "\n",
        "empty_count = 0\n",
        "results = []\n",
        "for i, image_path in enumerate(image_files):\n",
        "    print(f\"Processing {i+1}/{len(image_files)}: {image_path.name}\", end=\"\")\n",
        "\n",
        "    result = process_image(image_path, model, processor)\n",
        "\n",
        "    if not result.get(\"raw_text\"):\n",
        "        empty_count += 1\n",
        "        print(\" - WARNING: no text extracted!\", end=\"\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Create output structure\n",
        "    output = {\n",
        "        \"filename\": image_path.name,\n",
        "        \"model_name\": \"glm-ocr\",\n",
        "        \"raw_text\": result.get(\"raw_text\", \"\"),\n",
        "        \"processing_time_seconds\": result.get(\"processing_time_seconds\", 0),\n",
        "        \"file_size_bytes\": image_path.stat().st_size,\n",
        "        \"success\": result.get(\"success\", False)\n",
        "    }\n",
        "\n",
        "    if not result[\"success\"]:\n",
        "        output[\"error\"] = result.get(\"error\", \"Unknown error\")\n",
        "\n",
        "    results.append(output)\n",
        "\n",
        "    # Save individual result\n",
        "    result_file = OUTPUT_DIR / f\"{image_path.stem}_{image_path.suffix.lstrip('.')}.json\"\n",
        "    with open(result_file, \"w\") as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "\n",
        "    # Save raw response\n",
        "    raw_file = OUTPUT_DIR / \"raw\" / f\"{image_path.stem}_{image_path.suffix.lstrip('.')}_raw.json\"\n",
        "    with open(raw_file, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"content\": result.get(\"raw_text\", \"\"),\n",
        "            \"detections\": result.get(\"raw_data\", [])\n",
        "        }, f, indent=2)\n",
        "\n",
        "print(f\"\\nCompleted! Processed {len(results)} files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Save combined results and summary\n",
        "all_results_file = OUTPUT_DIR / \"all_results.json\"\n",
        "with open(all_results_file, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Print summary\n",
        "successful = sum(1 for r in results if r.get(\"success\", False))\n",
        "with_text = sum(1 for r in results if r.get(\"raw_text\", \"\"))\n",
        "total_time = sum(r.get(\"processing_time_seconds\", 0) for r in results)\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Total files: {len(results)}\")\n",
        "print(f\"  Successful: {successful}\")\n",
        "print(f\"  With text extracted: {with_text}\")\n",
        "print(f\"  Empty results: {empty_count}\")\n",
        "print(f\"  Failed: {len(results) - successful}\")\n",
        "print(f\"  Total processing time: {total_time:.1f}s\")\n",
        "print(f\"  Average time per file: {total_time/len(results):.2f}s\")\n",
        "print(f\"\\nResults saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Create downloadable ZIP\n",
        "import shutil\n",
        "\n",
        "shutil.make_archive(\n",
        "    \"/kaggle/working/glm_ocr\",\n",
        "    'zip',\n",
        "    OUTPUT_DIR\n",
        ")\n",
        "print(\"ZIP created: /kaggle/working/glm_ocr.zip\")\n",
        "print(\"Download from the Output tab after notebook completes.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
