{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NuMarkdown-8B-Thinking Invoice Processing\n",
    "\n",
    "> **WARNING: This notebook requires 16+ GB VRAM and may not run on Kaggle's free GPU tier (P100 16GB).**\n",
    ">\n",
    "> The 8B parameter model with BF16 precision requires significant VRAM. If you encounter OOM errors, consider:\n",
    "> - Using a GPU with more VRAM (A100, H100, RTX 4090)\n",
    "> - Using a quantized version of the model\n",
    ">\n",
    "> **Recommended GPUs:** NVIDIA A100 (40GB+), H100 (80GB), A10 (24GB), or RTX 4090 (24GB)\n",
    "\n",
    "This notebook processes invoice images using NuMind AI's NuMarkdown-8B-Thinking model.\n",
    "\n",
    "**Model:** [numind/NuMarkdown-8B-Thinking](https://huggingface.co/numind/NuMarkdown-8B-Thinking)\n",
    "\n",
    "**Key Features:**\n",
    "- First reasoning-based OCR VLM\n",
    "- Generates `<think>` tags to analyze document layout before producing markdown\n",
    "- Particularly good at complex layouts and tables\n",
    "- Fine-tuned from Qwen 2.5-VL-7B with RL (GRPO) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "# NuMarkdown-8B-Thinking is based on Qwen 2.5-VL and requires recent transformers\n",
    "# qwen-vl-utils provides utilities for Qwen VL models\n",
    "# pypdfium2 is needed for PDF-to-image conversion (model works on images only)\n",
    "!pip install transformers>=4.45.0 accelerate --quiet\n",
    "!pip install qwen-vl-utils --quiet\n",
    "!pip install pypdfium2 Pillow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Verify GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Setup and initialize NuMarkdown-8B-Thinking\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "# Reduce VRAM fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Configuration\n",
    "INPUT_DIR = Path(\"/kaggle/input/synthetic-invoices\")\n",
    "OUTPUT_DIR = Path(\"/kaggle/working/numarkdown-8b-thinking\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / \"raw\").mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"numind/NuMarkdown-8B-Thinking\"\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "# Initialize NuMarkdown-8B-Thinking\n",
    "# - Uses Qwen2_5_VLForConditionalGeneration (based on Qwen 2.5-VL-7B)\n",
    "# - flash_attention_2 for efficient attention computation\n",
    "# - BF16 precision for optimal memory/quality tradeoff\n",
    "print(\"Loading NuMarkdown-8B-Thinking model...\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    min_pixels=100*28*28,\n",
    "    max_pixels=5000*28*28\n",
    ")\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"NuMarkdown-8B-Thinking loaded on {DEVICE}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define processing function\n",
    "import pypdfium2 as pdfium\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "\n",
    "def pdf_to_image_path(pdf_path):\n",
    "    \"\"\"Convert first page of a PDF to a temporary PNG image. Returns the temp file path.\"\"\"\n",
    "    pdf = pdfium.PdfDocument(str(pdf_path))\n",
    "    page = pdf[0]\n",
    "    bitmap = page.render(scale=2)  # Higher scale for better OCR quality\n",
    "    pil_image = bitmap.to_pil()\n",
    "    tmp = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
    "    tmp_path = tmp.name\n",
    "    tmp.close()\n",
    "    pil_image.save(tmp_path)\n",
    "    return tmp_path\n",
    "\n",
    "def parse_output(result_text):\n",
    "    \"\"\"Parse the model output to extract reasoning and answer sections.\"\"\"\n",
    "    reasoning = \"\"\n",
    "    answer = \"\"\n",
    "    \n",
    "    # Extract <think>...</think> section\n",
    "    if \"<think>\" in result_text and \"</think>\" in result_text:\n",
    "        try:\n",
    "            reasoning = result_text.split(\"<think>\")[1].split(\"</think>\")[0].strip()\n",
    "        except IndexError:\n",
    "            pass\n",
    "    \n",
    "    # Extract <answer>...</answer> section\n",
    "    if \"<answer>\" in result_text and \"</answer>\" in result_text:\n",
    "        try:\n",
    "            answer = result_text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
    "        except IndexError:\n",
    "            pass\n",
    "    \n",
    "    # If no answer tags found, use the full text (minus thinking)\n",
    "    if not answer:\n",
    "        if \"</think>\" in result_text:\n",
    "            answer = result_text.split(\"</think>\")[-1].strip()\n",
    "        else:\n",
    "            answer = result_text\n",
    "    \n",
    "    return reasoning, answer\n",
    "\n",
    "def process_image(image_path, model, processor):\n",
    "    \"\"\"Process a single image or PDF with NuMarkdown-8B-Thinking.\"\"\"\n",
    "    start_time = time.time()\n",
    "    temp_file = None\n",
    "\n",
    "    try:\n",
    "        # Convert PDF to image if needed\n",
    "        if str(image_path).lower().endswith(\".pdf\"):\n",
    "            temp_file = pdf_to_image_path(image_path)\n",
    "            image = Image.open(temp_file).convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.open(str(image_path)).convert(\"RGB\")\n",
    "\n",
    "        # Build chat-format message (NuMarkdown uses Qwen's chat template)\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        }]\n",
    "        \n",
    "        # Apply chat template\n",
    "        prompt = processor.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Process image and text\n",
    "        model_input = processor(\n",
    "            text=prompt,\n",
    "            images=[image],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Generate output with reasoning\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **model_input,\n",
    "                temperature=0.7,\n",
    "                max_new_tokens=5000\n",
    "            )\n",
    "\n",
    "        # Decode the generated text\n",
    "        generated_text = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse reasoning and answer from the output\n",
    "        reasoning, answer = parse_output(generated_text)\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"raw_text\": generated_text,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"answer\": answer,\n",
    "            \"raw_data\": [{\"text\": generated_text, \"reasoning\": reasoning, \"answer\": answer}],\n",
    "            \"processing_time_seconds\": round(processing_time, 3)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"{str(e)}\\n{traceback.format_exc()}\",\n",
    "            \"raw_text\": \"\",\n",
    "            \"reasoning\": \"\",\n",
    "            \"answer\": \"\",\n",
    "            \"raw_data\": [],\n",
    "            \"processing_time_seconds\": time.time() - start_time\n",
    "        }\n",
    "    finally:\n",
    "        # Free GPU memory between images to prevent OOM\n",
    "        torch.cuda.empty_cache()\n",
    "        # Clean up temp PDF-converted image\n",
    "        if temp_file and os.path.exists(temp_file):\n",
    "            os.unlink(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Test on a single image before batch processing\n",
    "image_files = sorted(\n",
    "    list(INPUT_DIR.glob(\"*.png\")) +\n",
    "    list(INPUT_DIR.glob(\"*.jpeg\")) +\n",
    "    list(INPUT_DIR.glob(\"*.pdf\"))\n",
    ")\n",
    "print(f\"Found {len(image_files)} files total\")\n",
    "\n",
    "if image_files:\n",
    "    test_img = image_files[0]\n",
    "    print(f\"\\nTesting on: {test_img.name}\")\n",
    "\n",
    "    test_result = process_image(test_img, model, processor)\n",
    "    print(f\"Success: {test_result['success']}\")\n",
    "    print(f\"Processing time: {test_result['processing_time_seconds']:.2f}s\")\n",
    "\n",
    "    if test_result[\"answer\"]:\n",
    "        preview = test_result[\"answer\"][:500]\n",
    "        print(f\"\\nAnswer preview ({len(test_result['answer'])} chars):\\n{preview}\")\n",
    "        if test_result[\"reasoning\"]:\n",
    "            print(f\"\\nReasoning preview ({len(test_result['reasoning'])} chars):\\n{test_result['reasoning'][:300]}...\")\n",
    "        print(\"\\nOCR pipeline is working correctly!\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: No answer extracted!\")\n",
    "        if test_result.get(\"error\"):\n",
    "            print(f\"Error: {test_result['error']}\")\n",
    "else:\n",
    "    print(\"ERROR: No images found in input directory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Process all images\n",
    "print(f\"Processing {len(image_files)} files...\")\n",
    "\n",
    "empty_count = 0\n",
    "results = []\n",
    "for i, image_path in enumerate(image_files):\n",
    "    print(f\"Processing {i+1}/{len(image_files)}: {image_path.name}\", end=\"\")\n",
    "\n",
    "    result = process_image(image_path, model, processor)\n",
    "\n",
    "    if not result.get(\"answer\"):\n",
    "        empty_count += 1\n",
    "        print(\" - WARNING: no answer extracted!\", end=\"\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Create output structure\n",
    "    output = {\n",
    "        \"filename\": image_path.name,\n",
    "        \"model_name\": \"numarkdown-8b-thinking\",\n",
    "        \"raw_text\": result.get(\"raw_text\", \"\"),\n",
    "        \"reasoning\": result.get(\"reasoning\", \"\"),\n",
    "        \"answer\": result.get(\"answer\", \"\"),\n",
    "        \"processing_time_seconds\": result.get(\"processing_time_seconds\", 0),\n",
    "        \"file_size_bytes\": image_path.stat().st_size,\n",
    "        \"success\": result.get(\"success\", False)\n",
    "    }\n",
    "\n",
    "    if not result[\"success\"]:\n",
    "        output[\"error\"] = result.get(\"error\", \"Unknown error\")\n",
    "\n",
    "    results.append(output)\n",
    "\n",
    "    # Save individual result\n",
    "    result_file = OUTPUT_DIR / f\"{image_path.stem}_{image_path.suffix.lstrip('.')}.json\"\n",
    "    with open(result_file, \"w\") as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "\n",
    "    # Save raw response\n",
    "    raw_file = OUTPUT_DIR / \"raw\" / f\"{image_path.stem}_{image_path.suffix.lstrip('.')}_raw.json\"\n",
    "    with open(raw_file, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"content\": result.get(\"raw_text\", \"\"),\n",
    "            \"reasoning\": result.get(\"reasoning\", \"\"),\n",
    "            \"answer\": result.get(\"answer\", \"\"),\n",
    "            \"detections\": result.get(\"raw_data\", [])\n",
    "        }, f, indent=2)\n",
    "\n",
    "print(f\"\\nCompleted! Processed {len(results)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save combined results and summary\n",
    "all_results_file = OUTPUT_DIR / \"all_results.json\"\n",
    "with open(all_results_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "successful = sum(1 for r in results if r.get(\"success\", False))\n",
    "with_answer = sum(1 for r in results if r.get(\"answer\", \"\"))\n",
    "with_reasoning = sum(1 for r in results if r.get(\"reasoning\", \"\"))\n",
    "total_time = sum(r.get(\"processing_time_seconds\", 0) for r in results)\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total files: {len(results)}\")\n",
    "print(f\"  Successful: {successful}\")\n",
    "print(f\"  With answer extracted: {with_answer}\")\n",
    "print(f\"  With reasoning trace: {with_reasoning}\")\n",
    "print(f\"  Empty results: {empty_count}\")\n",
    "print(f\"  Failed: {len(results) - successful}\")\n",
    "print(f\"  Total processing time: {total_time:.1f}s\")\n",
    "print(f\"  Average time per file: {total_time/len(results):.2f}s\")\n",
    "print(f\"\\nResults saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create downloadable ZIP\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive(\n",
    "    \"/kaggle/working/numarkdown_8b_thinking\",\n",
    "    'zip',\n",
    "    OUTPUT_DIR\n",
    ")\n",
    "print(\"ZIP created: /kaggle/working/numarkdown_8b_thinking.zip\")\n",
    "print(\"Download from the Output tab after notebook completes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
